# Assessment Validation Methods
**Quality Assurance for Assessment Items**
**Source:** HMH Quality Standards + Assessment Best Practices
**Audience:** Curriculum developers creating and reviewing assessment items

---

## Overview

**Assessment validation ensures items are fair, accurate, reliable, aligned, accessible, and bias-free.** Every assessment item must pass through multiple validation layers before publication.

**Core Principle:** Invalid items produce invalid data. Rigorous validation protects instructional quality and student learning.

---

## The Validation Framework: 7 Dimensions

Every assessment item must be validated across 7 dimensions:

### 1. **Alignment** - Does it assess what it claims to assess?
### 2. **Accuracy** - Is the content mathematically correct?
### 3. **Clarity** - Will students understand what's being asked?
### 4. **Fairness** - Is it free from bias and accessible to all?
### 5. **Reliability** - Will it produce consistent results?
### 6. **Parity** - Do digital and print versions match?
### 7. **Technical Quality** - Does it function properly in delivery system?

---

## Dimension 1: Alignment Validation

### What to Validate:

**TEKS Alignment:**
- [ ] Item directly assesses the claimed TEKS standard
- [ ] All parts of the TEKS are represented
- [ ] No out-of-grade content required
- [ ] Student Expectation (SE) code is correct

**DOK Level:**
- [ ] Item complexity matches stated DOK level
- [ ] Cognitive demand is appropriate
- [ ] Task type matches DOK (not just difficulty)

**Learning Objective:**
- [ ] Item measures the specific objective
- [ ] Not just related—directly measures
- [ ] Context doesn't obscure the math

---

### Alignment Validation Process:

**Step 1: Read the Standard**
Example TEKS: "6.3C - represent integer operations with concrete models and connect the actions to the operations"

**Step 2: Identify Key Components**
- Represent operations (show visually/concretely)
- Integer operations (negative numbers)
- Connect actions to operations (explain relationship)

**Step 3: Check Item Coverage**
Does the item require students to:
- ✅ Represent? (draw model, use number line, etc.)
- ✅ Use integers? (includes negatives)
- ✅ Connect actions to operations? (explain what operation means)

**Step 4: Verify DOK Match**
- Stated DOK 2? Check if it requires skill application (not just recall)
- DOK should match the TEKS verb demand

---

### Common Alignment Errors:

**❌ Error 1: Too Narrow**
TEKS covers A, B, and C → Item only tests A

**❌ Error 2: Too Broad**
Item requires skills from multiple unrelated standards

**❌ Error 3: Wrong DOK**
Labeled DOK 2 but actually just recall (DOK 1)

**❌ Error 4: Context Mismatch**
Context requires knowledge beyond the math standard

---

## Dimension 2: Accuracy Validation

### What to Validate:

**Mathematical Correctness:**
- [ ] Correct answer is truly correct
- [ ] Work shown in key is accurate
- [ ] Formulas are stated correctly
- [ ] Vocabulary definitions are precise
- [ ] Representations are mathematically sound

**Rubric Accuracy:**
- [ ] 2-point descriptor matches full understanding
- [ ] 1-point accounts for common partial credit
- [ ] 0-point covers incorrect/no response
- [ ] Rubric aligns with what question asks

**Answer Key:**
- [ ] All acceptable answers listed
- [ ] Equivalent forms acknowledged
- [ ] Rounding specifications match stem
- [ ] Units match context

---

### Accuracy Validation Process:

**Step 1: Solve It Yourself**
- Solve without looking at provided answer
- Use multiple strategies if possible
- Check your work

**Step 2: Compare to Answer Key**
- Does your answer match?
- If not, determine who's wrong (you or the key)
- Verify with second person if uncertain

**Step 3: Check for Alternative Answers**
- Are there other valid forms? (fractions, decimals, mixed numbers)
- Different but equivalent expressions?
- Multiple valid interpretations?

**Step 4: Verify Rubric**
- Does "2 points" description match full correctness?
- Are partial credit scenarios realistic?
- Would this rubric produce fair scores?

---

### Common Accuracy Errors:

**❌ Error 1: Computational Mistake in Key**
Answer key shows 24 when correct answer is 42 (typo)

**❌ Error 2: Missing Equivalent Forms**
Key says "3/4" but doesn't accept "0.75" or "6/8"

**❌ Error 3: Rubric Mismatch**
Question asks for explanation, but 2-point rubric only mentions correct answer

**❌ Error 4: Context Contradiction**
Problem says "round to nearest tenth" but key shows whole number

---

## Dimension 3: Clarity Validation

### What to Validate:

**Stem Clarity:**
- [ ] Question is unambiguous
- [ ] Task is clear (what student must do)
- [ ] All necessary information provided
- [ ] No extraneous information (unless intentional)
- [ ] Grade-appropriate reading level
- [ ] Vocabulary is defined or familiar

**Visual Clarity:**
- [ ] Images are clear and labeled
- [ ] Diagrams have appropriate scale
- [ ] Graphs have axes labeled
- [ ] Text is readable size

**Direction Clarity:**
- [ ] Instructions are explicit
- [ ] "Show your work" stated if required
- [ ] "Explain" prompts specify what to explain
- [ ] Multi-part questions labeled clearly (Part A, Part B)

---

### Clarity Validation Process:

**Step 1: Read Aloud Test**
- Read the item stem aloud
- Can you understand what's being asked without re-reading?
- Are there confusing phrases?

**Step 2: Ambiguity Check**
- Could this be interpreted multiple ways?
- Is there only one clear task?
- Would two people read this the same way?

**Step 3: Grade-Level Check**
- Is vocabulary appropriate for grade?
- Is sentence structure accessible?
- Are numbers within grade-level range?

**Step 4: Test with Colleague**
- Have someone else read it
- Can they explain what students must do?
- Did they interpret it as you intended?

---

### Common Clarity Errors:

**❌ Error 1: Ambiguous Pronoun**
"When you add these, what do you get?" (add WHAT?)

**❌ Error 2: Unstated Expectation**
"Solve the equation" (but rubric requires showing work—should state this)

**❌ Error 3: Vocabulary Above Grade Level**
Grade 4 item uses "quotient" before it's been taught

**❌ Error 4: Unclear Referent**
"Use the model to solve it." (Which model? Solve what?)

---

## Dimension 4: Fairness Validation

### What to Validate:

**Bias-Free Content:**
- [ ] No stereotypes (race, gender, ability, culture, socioeconomic)
- [ ] Diverse representation in names and contexts
- [ ] No assumptions about family structure
- [ ] No assumptions about resources or experiences
- [ ] Cultural contexts are universal or explained
- [ ] No offensive or sensitive content

**Accessibility:**
- [ ] Readable by screen readers (if digital)
- [ ] Alt text provided for all images
- [ ] Color not the only means of conveying information
- [ ] No reliance on fine motor skills (unless assessed)
- [ ] Language supports available (EB considerations)

**Construct Relevance:**
- [ ] Reading level appropriate (not testing reading unless that's the goal)
- [ ] Context doesn't require privileged knowledge
- [ ] Cultural references are universal
- [ ] Math is the barrier, not the scenario

---

### Fairness Validation Process:

**Step 1: Representation Check**
- Review names, contexts, scenarios
- Is there diversity across race, gender, culture?
- Are portrayals stereotype-free?

**Step 2: Assumption Check**
- Does this assume students have certain experiences? (travel, technology, sports)
- Does this assume family structure? (two parents, siblings, etc.)
- Does this assume resources? (backyard, car, computer at home)

**Step 3: Accessibility Check**
- Can a blind student access this with screen reader?
- Can a student with limited fine motor skills respond?
- Can an EB student understand despite language complexity?

**Step 4: Sensitivity Check**
- Could any group find this offensive?
- Are sensitive topics avoided? (death, violence, religion, politics)
- IPACC compliance? (Texas-specific restrictions)

---

### Common Fairness Errors:

**❌ Error 1: Stereotypical Roles**
"Maria is baking. James is building." (gender stereotypes)

**❌ Error 2: Privileged Context**
"On your family's ski vacation..." (assumes wealth/opportunity)

**❌ Error 3: Inaccessible Visual**
Image with no alt text; blind student can't access

**❌ Error 4: Construct-Irrelevant Difficulty**
Testing ratios but context requires understanding cricket rules

---

## Dimension 5: Reliability Validation

### What to Validate:

**Consistent Scoring:**
- [ ] Rubric enables consistent scoring across teachers
- [ ] Clear distinction between score points
- [ ] Minimal subjectivity in evaluation
- [ ] Examples or anchor papers provided for complex items

**Item Consistency:**
- [ ] Similar items produce similar difficulty
- [ ] Students at same ability level perform similarly
- [ ] Item doesn't "trick" students

**Response Consistency:**
- [ ] Answer format is standardized
- [ ] Partial credit scenarios are predictable
- [ ] Students know how to respond

---

### Reliability Validation Process:

**Step 1: Rubric Application Test**
- Give 3-5 sample student responses to two people
- Have them score independently using rubric
- Do they give same scores? (Inter-rater reliability)
- If not, revise rubric for clarity

**Step 2: Boundary Case Check**
- Create responses that are "on the edge" (between 1 and 2 points)
- Can scorers consistently place these?
- If not, clarify rubric boundaries

**Step 3: Answer Format Check**
- Is it clear where/how student should respond?
- Could confusion about format affect scores?
- Are instructions consistent with other items?

---

### Common Reliability Errors:

**❌ Error 1: Vague Rubric**
"2 points: Good explanation" (what's "good"?)

**❌ Error 2: Overlapping Score Points**
Unclear when to give 1 vs. 2 points

**❌ Error 3: Subjective Criteria**
Scoring based on "neatness" or "effort" (not the math)

**❌ Error 4: Inconsistent Format**
Some items say "Show work," others don't, but rubrics expect it

---

## Dimension 6: Parity Validation

### What to Validate:

**Digital-Print Match:**
- [ ] **Level 1:** Question structure/layout matches
- [ ] **Level 2:** Answer structure/layout matches
- [ ] **Level 3:** Interaction is equivalent

**Student Experience:**
- [ ] Students can identify they're on same item
- [ ] Students know where to respond
- [ ] Students understand how to interact
- [ ] Cognitive demand is the same

**Technical Equivalence:**
- [ ] Both versions assess same skill
- [ ] Both versions have same difficulty
- [ ] Both versions produce comparable data

---

### Parity Validation Process:

**Use the 3-Question Test:**

**Question 1: Can student tell they're on the same item?**
- Read digital stem, read print stem
- Do they look/sound the same?
- Same images, same wording, same structure?

**Question 2: Does student know where to write answer?**
- Digital: Clear response area?
- Print: Clear response space?
- Same format (blank, box, lines)?

**Question 3: Does student do same cognitive work?**
- Digital: Generate or select?
- Print: Generate or select?
- If different → does it change the math?

**Parity Level:**
- All 3 YES → Level 3 (Full Parity) ✅
- Question 1-2 YES, Question 3 NO → Level 2 ⚠️
- Question 1 NO → Level 1 or below ❌

---

### Common Parity Errors:

**❌ Error 1: Print Multiple Choice, Digital Drop Down**
Students told "Select all that apply" but print has bubbles (single select)

**❌ Error 2: Print Has Work Space, Digital Doesn't**
Different expectations for showing work

**❌ Error 3: Digital Auto-Formats, Print Doesn't**
Digital: Student enters "1/2", system displays as fraction
Print: Student must write fraction notation themselves (different skill)

**❌ Error 4: Image Quality Mismatch**
Digital has zoomable image; print has tiny, hard-to-read version

---

## Dimension 7: Technical Quality Validation

### What to Validate:

**Learnosity Functionality:**
- [ ] Question renders correctly in all browsers
- [ ] Response areas are appropriately sized
- [ ] Scoring logic is configured correctly
- [ ] Feedback (if any) displays properly
- [ ] Accessibility features work (screen reader, zoom)

**JSON Accuracy:**
- [ ] All required fields populated
- [ ] Validation rules configured correctly
- [ ] Response IDs are unique
- [ ] Metadata is complete

**User Experience:**
- [ ] Navigation is intuitive
- [ ] Load time is acceptable
- [ ] Mobile-responsive (if applicable)
- [ ] No confusing interface elements

---

### Technical Validation Process:

**Step 1: Preview in Learnosity**
- Load item in authoring system
- Check all elements display
- Test all interactive features
- View in multiple browsers

**Step 2: Test Scoring**
- Submit correct answer → should award full points
- Submit incorrect answer → should award 0 points
- Submit partial answer → should award 1 point (if applicable)
- Check scoring logic configuration

**Step 3: Test Accessibility**
- Turn on screen reader → can all content be read?
- Zoom to 200% → does layout still work?
- Navigate by keyboard only → can you reach all elements?

**Step 4: Review Metadata**
- TEKS code matches content?
- DOK level recorded?
- Item type classification correct?
- All tags applied?

---

### Common Technical Errors:

**❌ Error 1: Broken Image Links**
Image doesn't display (wrong path, file missing)

**❌ Error 2: Incorrect Scoring Logic**
Configured to accept only "0.5" but not "1/2" (should accept both)

**❌ Error 3: Missing Alt Text**
Image has no description for screen readers

**❌ Error 4: Validation Too Strict**
Won't accept valid equivalent answers (6/8 when expecting 3/4)

---

## The Complete Validation Checklist

### Pre-Review (Author Self-Check):

**Content:**
- [ ] Solved problem myself; answer is correct
- [ ] Checked for typos and grammatical errors
- [ ] Verified all images display correctly
- [ ] Confirmed grade-appropriate language

**Alignment:**
- [ ] TEKS code is correct
- [ ] DOK level matches cognitive demand
- [ ] Learning objective clearly assessed

**Clarity:**
- [ ] Question is unambiguous
- [ ] Task is clearly stated
- [ ] All necessary information present

**Fairness:**
- [ ] No stereotypes or bias
- [ ] Diverse representation
- [ ] No construct-irrelevant difficulty

---

### Peer Review (Colleague Check):

**Mathematical Accuracy:**
- [ ] Independently solved; answer confirmed
- [ ] Checked for alternative valid answers
- [ ] Verified rubric accuracy

**Alignment:**
- [ ] TEKS coverage verified
- [ ] DOK classification confirmed
- [ ] No out-of-grade content

**Clarity & Fairness:**
- [ ] Read aloud; clear and unambiguous
- [ ] No assumptions or biases noted
- [ ] Accessibility features present

---

### Technical Review (QA Check):

**Functionality:**
- [ ] Item renders in authoring system
- [ ] Interactive elements work
- [ ] Scoring logic tested with sample responses
- [ ] Accessibility features verified

**Parity:**
- [ ] Digital-print versions compared
- [ ] 3-question test passed
- [ ] Parity level documented

**Metadata:**
- [ ] All fields complete
- [ ] Tags applied
- [ ] Item bank organized correctly

---

### Final Review (Lead Review):

**Overall Quality:**
- [ ] All previous checks completed
- [ ] Item meets HMH quality standards
- [ ] Supports instructional goals
- [ ] Ready for pilot/field test

**Documentation:**
- [ ] Rationale documented (if needed)
- [ ] Known issues logged
- [ ] Version control updated

**Approval:**
- [ ] Approved by content lead
- [ ] Approved by assessment specialist
- [ ] Added to approved item bank

---

## Validation Workflow

### Stage 1: Author Creation & Self-Review
**Responsibility:** Item author
**Time:** 30-45 minutes per item
**Checklist:** Pre-Review section

**Output:** Draft item with self-review notes

---

### Stage 2: Peer Review
**Responsibility:** Colleague or team member
**Time:** 15-20 minutes per item
**Checklist:** Peer Review section

**Output:** Feedback for revision or approval to proceed

---

### Stage 3: Revision
**Responsibility:** Item author
**Time:** 10-30 minutes per item

**Output:** Revised item addressing feedback

---

### Stage 4: Technical Review
**Responsibility:** QA specialist or tech lead
**Time:** 15-20 minutes per item
**Checklist:** Technical Review section

**Output:** Technical approval or list of technical issues

---

### Stage 5: Final Review & Approval
**Responsibility:** Content lead or assessment director
**Time:** 10-15 minutes per item
**Checklist:** Final Review section

**Output:** Approved item ready for publication

---

### Stage 6: Pilot Testing (Optional but Recommended)
**Responsibility:** Research team
**Time:** Varies (classroom testing)

**Data Collected:**
- Item difficulty (% correct)
- Item discrimination (strong students vs. weak students)
- Time to complete
- Student feedback

**Output:** Data-informed refinements

---

## Quality Indicators: Good vs. Poor Items

### Characteristics of HIGH-QUALITY Items:

✅ **Aligned:** Directly assesses stated standard
✅ **Accurate:** Mathematically and pedagogically correct
✅ **Clear:** Unambiguous; students know what to do
✅ **Fair:** Accessible to all; bias-free
✅ **Reliable:** Consistent scoring; predictable difficulty
✅ **Parity:** Digital-print equivalence maintained
✅ **Technical:** Functions correctly; well-coded

**Example High-Quality Item:**

**Question (Grade 5, TEKS 5.3K - Divide whole numbers):**
"A bakery has 156 cookies. They pack them into boxes of 12. How many boxes can they fill completely? Show your work."

**Why High-Quality:**
- ✅ Aligned to TEKS 5.3K (division)
- ✅ Accurate: 156 ÷ 12 = 13 boxes
- ✅ Clear: Unambiguous question
- ✅ Fair: Universal context (bakery)
- ✅ Reliable: Straightforward rubric
- ✅ Grade-appropriate numbers and context

---

### Characteristics of LOW-QUALITY Items:

❌ **Misaligned:** Tests wrong standard or off-grade content
❌ **Inaccurate:** Mathematical errors or flawed rubric
❌ **Unclear:** Ambiguous or confusing
❌ **Unfair:** Bias, stereotypes, or accessibility issues
❌ **Unreliable:** Vague rubric; inconsistent scoring
❌ **Parity Problems:** Digital-print mismatch
❌ **Technical Issues:** Broken, won't render, poor UX

**Example Low-Quality Item:**

**Question (Grade 5):**
"On your family's trip to Paris, you bought 3 souvenirs for €15 each. If the exchange rate is 1.18, how much did you spend in dollars? Explain using the formula."

**Why Low-Quality:**
- ❌ Unfair: Assumes family travel, knowledge of Paris, familiarity with € symbol
- ❌ Misaligned: Requires multiplication, currency conversion, formula knowledge (likely off-grade)
- ❌ Unclear: Which formula? What exchange rate calculation method?
- ❌ Construct-irrelevant difficulty: Math is simple; context is complex

---

## Common Validation Pitfalls

### Pitfall 1: Skipping Self-Review
**Problem:** Submitting first draft without checking

**Impact:** Wastes reviewers' time catching basic errors

**Solution:** Always complete pre-review checklist before submitting

---

### Pitfall 2: Not Testing Items
**Problem:** Never actually solving the item or testing interaction

**Impact:** Errors in answer keys, broken functionality, poor UX

**Solution:** Solve every item yourself; test all digital features

---

### Pitfall 3: Ignoring Parity
**Problem:** Creating digital version without considering print

**Impact:** Two versions assess different things; unfair for print users

**Solution:** Always design both versions together; validate parity

---

### Pitfall 4: Weak Rubrics
**Problem:** Vague rubric criteria that don't enable consistent scoring

**Impact:** Unreliable scores; unfair grading

**Solution:** Use specific, observable criteria; test with sample responses

---

### Pitfall 5: Assumption Bias
**Problem:** Author assumes students have certain knowledge/experiences

**Impact:** Item is harder for some groups than others (not due to math)

**Solution:** Use universal contexts; explain cultural references; avoid assumptions

---

## Validation Tools and Resources

### Alignment Tools:
- TEKS Lookup: Official Texas standards documents
- DOK Framework: `/assessment/dok-framework.md`
- Content Block Schema: `/structure/content-block-schema.json`

### Accuracy Tools:
- Math verification calculators
- Peer solve-and-check
- Answer key templates

### Fairness Tools:
- CEID Guidelines: `/texas-compliance/ceid-guidelines.md` (pending)
- Bias review checklist
- Accessibility validator (WAVE, aXe)

### Parity Tools:
- Parity Guidelines: `/assessment/parity-guidelines.md`
- 3-Question Test
- Side-by-side comparison template

### Technical Tools:
- Learnosity Author Portal
- JSON validators
- Browser testing tools (BrowserStack)

---

## Quick Reference: Validation in 5 Minutes

When time is limited, prioritize these checks:

### The Essential 5:

1. **Solve it yourself** → Verify answer is correct
2. **Read aloud** → Ensure clarity
3. **Check TEKS code** → Verify alignment
4. **Look for bias** → Universal context? Diverse names?
5. **Test digitally** → Does it render and score correctly?

**These 5 catch 80% of common errors.**

---

## Resources

**Related Guides:**
- DOK Framework: `/assessment/dok-framework.md`
- Scoring Rubrics: `/assessment/scoring-rubrics-guide.md`
- Item Types Reference: `/assessment/item-types-reference.md`
- Parity Guidelines: `/assessment/parity-guidelines.md`

**Texas Compliance:**
- IPACC Requirements: `/texas-compliance/ipacc-suitability-requirements.md`
- SBOE Quality Rubric: `/texas-compliance/sboe-quality-rubric.md`
- Texas Compliance Checklist: `/texas-compliance/texas-compliance-checklist.md`

**Accessibility:**
- UDL Principles: `/udl/udl-principles-guide.md`
- Alt Text Guidelines: `/accessibility/alt-text-principles.md` (pending)

---

**Remember:** Validation is not optional—it's essential. An invalid item doesn't just waste students' time; it produces bad data that leads to bad instructional decisions. Invest in validation to ensure every item is worthy of students' efforts.
